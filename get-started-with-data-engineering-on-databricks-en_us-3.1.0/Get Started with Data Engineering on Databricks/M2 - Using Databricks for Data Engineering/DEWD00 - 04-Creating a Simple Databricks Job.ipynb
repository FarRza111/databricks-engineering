{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b5654f4-0895-4746-ae32-2a4930fd7fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f82fa6d-94e8-49c7-8674-a3a0a6dda70f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creating a Simple Databricks Job\n",
    "\n",
    "Databricks Jobs (Lakeflow Jobs) provides a collection of tools that allow you to schedule and orchestrate all processing tasks on Databricks.\n",
    "\n",
    "**Objective:** Use the pipeline built in the previous demonstration to create two tasks in a job. The pipeline has been separated into two notebooks for demonstration purposes:\n",
    "- **DEWD00 - 04A-Task 1 - Setup - Bronze**\n",
    "- **DEWD00 - 04B-Task 2 - Silver - Gold**\n",
    "\n",
    "\n",
    "**NOTE:** You could have used a Lakeflow Spark Declarative Pipeline for this data engineering task, but Spark Declarative Pipeline is beyond the scope of this course. SDP can be scheduled within a Lakeflow Job with additional tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1a173cd-1431-4b35-9b8f-a28096456d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87222129-3cd2-4850-9925-1238d9d0a7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a58aec57-8356-4e7c-9e90-d0ff520cd876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80c896ae-0a76-469c-a92b-1028740b2771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1. Generate Lakeflow Job Configuration\n",
    "\n",
    "Configuring this lakeflow job will require parameters unique to a given user.\n",
    "\n",
    "Run the cell below to print out values you'll use to configure your lakeflow job in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa14dabb-c303-49ad-b862-661a5908a2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "DA.print_lakeflow_job_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87c974be-dcc2-4b89-a61f-fe008e00592a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2. Configure Job with a Notebook Task\n",
    "\n",
    "When using the Jobs UI to orchestrate a workload with multiple tasks, you'll always begin by creating a job with a single task, and can add more if required.\n",
    "\n",
    "Complete the following to create a lakeflow job with two tasks using the notebooks from above (**DEWD00 - 04A-Task 1 - Setup - Bronze** and **DEWD00 - 04B-Task 2 - Silver - Gold**):\n",
    "\n",
    "1. Right-click the **Jobs & Pipelines** button on the sidebar, and open the link in a new tab. This way, you can refer to these instructions, as needed.\n",
    "\n",
    "2. Confirm you are in the **Jobs & Pipelines** tab.\n",
    "\n",
    "3. On the right side, select **Create -> Job**.\n",
    "\n",
    "4. In the top-left of the screen, enter the **Job Name** provided above to add a name for the lakeflow job.\n",
    "\n",
    "5. Under **Add your first task**, select **Notebook**. If **Notebook** is not listed, click **+ Add another task type** and choose **Notebook** from the options.\n",
    "\n",
    "6. Follow the instructions below to set up your job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34b83c25-7547-44eb-b3cc-cb87785a43fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Create Task 1\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| Task name | Enter **Setup-Bronze** |\n",
    "| Type | Ensure **Notebook** is selected. Note in the dropdown list the many different types of lakeflow jobs that can be scheduled |\n",
    "| Source | Ensure **Workspace** is selected |\n",
    "| Path | Use the navigator to specify the **DEWD00 - 04A-Task 1 - Setup - Bronze** notebook. Use the path from above to help find the notebook. |\n",
    "| **Compute**     | Select a **Serverless** cluster from the dropdown menu.<br>(We will use Serverless clusters for all jobs in this course. You may specify a different cluster outside of this course, if needed.) <br></br> **NOTE**: When selecting your all-purpose cluster, you may get a warning about how this will be billed as all-purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate.|\n",
    "| Environment and Libraries| Ensure **Default** is selected |\n",
    "| Create | Select the **Create task** button to create the task |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac265c70-a8df-4f5b-8ce7-3d7c8de6a8f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Task 2\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| New task | Select **Add task** within your job. Then select **Notebook**|\n",
    "| Task name | Enter **Silver-Gold** |\n",
    "| Type | Choose **Notebook**. Note in the dropdown list the many different types of lakeflow jobs that can be scheduled |\n",
    "| Source | Choose **Workspace** |\n",
    "| Path | Use the navigator to specify the **DEWD00 - 04B-Task 2 - Silver - Gold** notebook. Use the path from above to help find the notebook. |\n",
    "| **Compute**     | Select a **Serverless** cluster from the dropdown menu.<br>(We will use Serverless clusters for all jobs in this course. You may specify a different cluster outside of this course, if needed.) <br></br> **NOTE**: When selecting your all-purpose cluster, you may get a warning about how this will be billed as all-purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate.|\n",
    "| Depends on | Select **setup-Bronze** |\n",
    "| Run if dependencies | Select **All succeeded** |\n",
    "| Environment and Libraries| Ensure **Default** is selected |\n",
    "| Create | Select the **Create task** button to create the task |\n",
    "\n",
    "##### For better performance, please enable Performance Optimized Mode in Job Details. Otherwise, it might take 6 to 8 minutes to initiate execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7840d05d-586e-4646-890f-81595313c619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## 3. Explore Scheduling Options\n",
    "Complete the following steps to explore the scheduling options:\n",
    "\n",
    "1. On the right hand side of the Jobs UI, locate the **Schedules & Triggers** section.\n",
    "\n",
    "2. Select the **Add trigger** button to explore scheduling options.\n",
    "\n",
    "3. Changing the **Trigger type** from **None (Manual)** to **Scheduled** will bring up a cron scheduling UI.\n",
    "\n",
    "   - This UI provides extensive options for setting up chronological scheduling of your LakeFlow Jobs. Settings configured with the UI can also be output in cron syntax, which can be edited if custom configuration is not available when the UI is needed.\n",
    "\n",
    "4. Select **Cancel** to return to Job details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f73552a2-978d-4cc1-a799-1bfd4cdf8859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Run Job\n",
    "Select **Run now** above  **Job details** to execute the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e36ae1e-99a0-470b-a73f-1e06b080faa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Review Job Run\n",
    "\n",
    "To review the job run:\n",
    "\n",
    "1. On the Job details page, select the **Runs** tab in the top-left of the screen (you should currently be on the **Tasks** tab)\n",
    "\n",
    "2. Find your job.\n",
    "\n",
    "3. Open the output details by clicking on the timestamp field under the **Start time** column\n",
    "\n",
    "    - If **the job is still running**, you will see the active state of the notebook with a **Status** of **`Pending`** or **`Running`** in the right side panel.\n",
    "\n",
    "    - If **the job has completed**, you will see the full execution of the notebook with a **Status** of **`Succeeded`** or **`Failed`** in the right side panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f3ffdb6-5130-4d5c-9faf-2604d4d53d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7820632833899514,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DEWD00 - 04-Creating a Simple Databricks Job",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "SQL",
   "language": "sql",
   "name": "sql"
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
