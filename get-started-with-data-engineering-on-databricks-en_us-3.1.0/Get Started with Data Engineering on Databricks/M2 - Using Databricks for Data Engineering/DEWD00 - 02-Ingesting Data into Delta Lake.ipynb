{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "769db0d3-9069-4cbf-86eb-6899d6366a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebb60167-1a7c-43fd-8c27-25b4e873f176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingesting Data into Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79e72e89-4954-4421-9960-193f2d8f216b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faff6225-d65e-497d-bb4c-57f8529638a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0021cc53-7d44-477d-960a-5e8a661fc25c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1c1ca86-2053-44ca-98e5-191b98ffa7b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Configure and Explore Your Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "931841fa-8b28-424e-b8f3-8dc4de54923c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Setting Up catalog and Schema\n",
    "Set the default catalog to **dbacademy** and your unique schema. Then, view the available tables to confirm that no tables currently exist in your schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0f91f05-7388-4579-ad6e-c0307eceb6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1A. Using SQL Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "750f6fbd-a091-43bb-884c-949bb3978f29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Set the default catalog and schema\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA IDENTIFIER(DA.schema_name);\n",
    "\n",
    "-- Display available tables in your schema\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e72f3a8f-af76-4371-8432-00303a50d004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1B. Using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c6673cd-c3a9-41dd-aacb-e10929343f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the default catalog and schema (Requires Spark 3.4.0 or later)\n",
    "spark.catalog.setCurrentCatalog(DA.catalog_name)\n",
    "spark.catalog.setCurrentDatabase(DA.schema_name)\n",
    "\n",
    "# Display available tables in your schema\n",
    "spark.catalog.listTables(DA.schema_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e53bea8b-dc17-41a3-b52d-fe5837421878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Viewing the available files\n",
    "View the available files in your schema's **myfiles** volume. Confirm that only the **employees.csv** file is available.\n",
    "\n",
    "**NOTE:** Remember, when referencing data in volumes, use the path provided by Unity Catalog, which always has the following format: */Volumes/catalog_name/schema_name/volume_name/*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff8411b2-fe59-44b5-88cc-b5667efd1d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"LIST '/Volumes/dbacademy/{DA.schema_name}/myfiles/' \").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "034dad55-0031-4393-8c00-281c975c183f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Delta Lake Ingestion Techniques\n",
    "**Objective**: Create a Delta table from the **employees.csv**  file using various methods.\n",
    "\n",
    "- CREATE TABLE AS (`CTAS`)\n",
    "- UPLOAD UI (`User Interface`)\n",
    "- COPY INTO\n",
    "- AUTOLOADER (`Overview only`, `outside the scope of this module`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db2a5336-8a9b-4677-94fe-8e7c73961c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. CREATE TABLE (CTAS)\n",
    "1. Create a table from the **employees.csv** file using the CREATE TABLE AS statement similar to the previous demonstration. Run the query and confirm that the **current_employees_ctas** table was successfully created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4c9016e-d4fd-43fa-8ee4-85f02de71450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS current_employees_ctas;\n",
    "\n",
    "-- Create the table using CTAS\n",
    "CREATE TABLE current_employees_ctas\n",
    "AS\n",
    "SELECT ID, FirstName, Country, Role \n",
    "FROM read_files(\n",
    "  '/Volumes/dbacademy/' || DA.schema_name || '/myfiles/',\n",
    "  format => 'csv',\n",
    "  header => true,\n",
    "  inferSchema => true\n",
    " );\n",
    "\n",
    "-- Display available tables in your schema\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d719f7c5-e70a-40ad-a36f-e3e6d5e3f765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Query the **current_employees_ctas** table and confirm that it contains 4 rows and 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9414eb31-10ab-479c-9a3e-bcdc4310a996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM current_employees_ctas;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a7e9019-0097-4978-8559-f3a181eb547a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. UPLOAD UI\n",
    "The add data UI allows you to manually load data into Databricks from a variety of sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6d3662f-37e8-4afb-b9f9-0aac599e08ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Complete the following steps to manually download the **employees.csv** file from your volume:\n",
    "\n",
    "   a. Select the Catalog icon in the left navigation bar. \n",
    "\n",
    "   b. Click on your catalog **(dbacademy)**\n",
    "\n",
    "   c. Select the refresh icon to refresh the **dbacademy** catalog.\n",
    "\n",
    "   d. Expand the **dbacademy** catalog. Within the catalog, you should see a variety of schemas (databases).\n",
    "\n",
    "   e. Expand your schema. You can locate your schema in the setup notes in the first cell. Notice that your schema contains **Tables** and **Volumes**.\n",
    "\n",
    "   f. Expand **Volumes** then **myfiles**. The **myfiles** volume should contain a single CSV file named **employees.csv**. \n",
    "\n",
    "   g. Click on the kebab menu on the right-hand side of the **employees.csv** file and select **Download Volume file.** This will download the CSV file to your browser's download folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05d7dc08-8ef5-45f1-b994-bf5ad29eef9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Complete the following steps to manually upload the **employees.csv** file to your schema. This will mimic loading a local file to Databricks:\n",
    "\n",
    "   a. In the navigation bar select your schema. \n",
    "\n",
    "   b. Click the ellipses (three-dot) icon next to your schema and select **Open in Catalog Explorer**.\n",
    "\n",
    "   c. Select the **Create** drop down icon ![create_drop_down](../Includes/images/create_drop_down.png), and select **Table**.\n",
    "\n",
    "   d. Select the **employees.csv** you downloaded earlier into the available section in the browser, or select **browse**, navigate to your downloads folder and select the **employees.csv** file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fac1c802-17ee-483f-ae7f-b319745ea6da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Complete the following steps to create the Delta table using the UPLOAD UI.\n",
    "\n",
    "   a. In the UI confirm the table will be created in the catalog **dbacademy** and your unique schema. \n",
    "\n",
    "   b. Under **Table name**, name the table **current_employees_ui**.\n",
    "\n",
    "   c. Select the **Create table** button at the bottom of the screen to create the table.\n",
    "\n",
    "   d. Confirm the table was created successfully. Then close out of the Catalog Explorer browser.\n",
    "\n",
    "**Example**\n",
    "<br></br>\n",
    "\n",
    "![create_table_ui](../Includes/images/create_table_ui.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b02d62e4-8f46-4f63-8770-9398fb07a5ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Use the SHOW TABLES statement to view the available tables in your schema. Confirm that the **current_employees_ui** table has been created. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90200715-8d75-4d83-b173-3e21663d626d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "762020d3-cc20-4f8a-9b4f-966c7e2efe6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Lastly, query the table to review its contents.\n",
    "\n",
    "**NOTE**: If you did not upload the table using the UPLOAD UI and name it **current_employees_ui** an error will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2347775b-318a-43ba-93ac-cd9fc85402e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM current_employees_ui;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec4a0727-9303-4708-94a4-f2896706d572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. COPY INTO\n",
    "Create a table from the **employees.csv** file using the [COPY INTO](https://docs.databricks.com/en/sql/language-manual/delta-copy-into.html) statement. \n",
    "\n",
    "The `COPY INTO` statement incrementally loads data from a file location into a Delta table. This is a retryable and idempotent operation. Files in the source location that have already been loaded are skipped. This is true even if the files have been modified since they were loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66b3173e-5aa7-474e-bdae-15f72598aaa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Create an empty table named **current_employees_copyinto** and define the column data types.\n",
    "\n",
    "**NOTE:** You can also create an empty table with no columns and evolve the schema with `COPY INTO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10b437e6-5c98-415a-b23e-4c35cff4d4e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS current_employees_copyinto;\n",
    "\n",
    "-- Create an empty table with the column data types\n",
    "CREATE TABLE current_employees_copyinto (\n",
    "  ID INT,\n",
    "  FirstName STRING,\n",
    "  Country STRING,\n",
    "  Role STRING\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "267c1d23-8a61-4044-a67a-6357072f7f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Use the `COPY INTO` statement to load all files from the **myfiles** volume (currently only the **employees.csv** file exists) using the path provided by Unity Catalog. Confirm that the data is loaded into the **current_employees_copyinto** table.\n",
    "\n",
    "    Confirm the following:\n",
    "    - **num_affected_rows** is 4\n",
    "    - **num_inserted_rows** is 4\n",
    "    - **num_skipped_correct_files** is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdce3615-c305-4e92-a379-77a258b9d9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "COPY INTO current_employees_copyinto\n",
    "  FROM '/Volumes/dbacademy/{DA.schema_name}/myfiles/'\n",
    "  FILEFORMAT = CSV\n",
    "  FORMAT_OPTIONS (\n",
    "      'header' = 'true', \n",
    "      'inferSchema' = 'true'\n",
    "    )\n",
    "  ''').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "084b7f86-a3a4-4072-acb9-aecb72c4a735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Query the **current_employees_copyinto** table and confirm that all 4 rows have been copied into the Delta table correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1442948-ea34-4247-b012-ad18ba20c265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM current_employees_copyinto;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21bf7d09-0ca0-4271-a8f8-99086e020438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Run the `COPY INTO` statement again and confirm that it did not re-add the data from the volume that was already loaded. Remember, `COPY INTO` is a retryable and idempotent operation â€” Files in the source location that have already been loaded are skipped.   \n",
    "    - **num_affected_rows** is 0\n",
    "    - **num_inserted_rows** is 0\n",
    "    - **num_skipped_correct_files** is 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efc290d1-c823-46dd-aa5c-3a5696727e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "COPY INTO current_employees_copyinto\n",
    "  FROM '/Volumes/dbacademy/{DA.schema_name}/myfiles/'\n",
    "  FILEFORMAT = CSV\n",
    "  FORMAT_OPTIONS (\n",
    "      'header' = 'true', \n",
    "      'inferSchema' = 'true'\n",
    "    )\n",
    "  ''').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "941c2a3b-985f-4680-bcc9-b7ce95e5e4f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Run the script below to create an additional CSV file named **employees2.csv** in your **myfiles** volume. View the results and confirm that your volume now contains two CSV files: the original **employees.csv** file and the new **employees2.csv** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28b5e4a7-d682-452a-a8b8-cece97ad1964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Create the new employees2.csv file in your volume\n",
    "DA.create_employees_csv2()\n",
    "\n",
    "## View the files in the your myfiles volume\n",
    "files = dbutils.fs.ls(f'/Volumes/dbacademy/{DA.schema_name}/myfiles')\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abc88214-04e5-4d15-bfd6-fccda55e9ab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Query the new **employees2.csv** file directly. Confirm that only 2 rows exist in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bff482a-ded5-4b3d-960a-c1f3a6095128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  ID, \n",
    "  FirstName, \n",
    "  Country, \n",
    "  Role \n",
    "FROM read_files(\n",
    "  '/Volumes/dbacademy/' || DA.schema_name || '/myfiles/employees2.csv',\n",
    "  format => 'csv',\n",
    "  header => true,\n",
    "  inferSchema => true\n",
    " );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02038e35-8af7-480f-bdb6-0985ecb8757b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Execute the `COPY INTO` statement again using your volume's path. Notice that only the 2 rows from the new **employees2.csv** file are added to the **current_employees_copyinto** table.\n",
    "\n",
    "    - **num_affected_rows** is 2\n",
    "    - **num_inserted_rows** is 2\n",
    "    - **num_skipped_correct_files** is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2748ddcb-2531-4575-80b7-2a43130ab3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "COPY INTO current_employees_copyinto\n",
    "  FROM '/Volumes/{DA.catalog_name}/{DA.schema_name}/myfiles/'\n",
    "  FILEFORMAT = CSV\n",
    "  FORMAT_OPTIONS (\n",
    "      'header' = 'true', \n",
    "      'inferSchema' = 'true'\n",
    "    )\n",
    "  ''').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1db7ee1-7475-4f36-a17f-08d2ef8c74f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. View the updated **current_employees_copyinto** table and confirm that it now contains 6 rows, including the new data that was added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1e09d31-f47b-4048-8228-554e711ada61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM current_employees_copyinto;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed7f0971-330b-4840-85ad-866b0e8410d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. View table's history. Notice that there are 3 versions.\n",
    "    - **Version 0** is the initial empty table created by the CREATE TABLE statement.\n",
    "    - **Version 1** is the first `COPY INTO` statement that loaded the **employees.csv** file into the Delta table.\n",
    "    - **Version 2** is the second `COPY INTO` statement that only loaded the new **employees2.csv** file into the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "018578b5-113d-42bf-907b-6707d8c98c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY current_employees_copyinto;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a98e6be-0bc1-4a3d-b96f-add3081a8000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####4. AUTO LOADER\n",
    "\n",
    "**NOTE: Auto Loader is outside the scope of this course.**\n",
    "\n",
    "Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional setup.\n",
    "\n",
    "![autoloader](../Includes/images/autoloader.png)\n",
    "\n",
    "The key benefits of using the auto loader are:\n",
    "- No file state management: The source incrementally processes new files as they land on cloud storage. You don't need to manage any state information on what files arrived.\n",
    "- Scalable: The source will efficiently track the new files arriving by leveraging cloud services and RocksDB without having to list all the files in a directory. This approach is scalable even with millions of files in a directory.\n",
    "- Easy to use: The source will automatically set up notification and message queue services required for incrementally processing the files. No setup needed on your side.\n",
    "\n",
    "Check out the documentation\n",
    "[What is Auto Loader](https://docs.databricks.com/en/ingestion/auto-loader/index.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5ec6941-ccdc-42b8-a5a0-85b8d09dafa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Cleanup\n",
    "1. Drop your demonstration tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "668d57f0-f034-429d-88e8-47c233931529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS current_employees_ctas;\n",
    "DROP TABLE IF EXISTS current_employees_ui;\n",
    "DROP TABLE IF EXISTS current_employees_copyinto;\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "820aa0e8-60ce-4b2a-875a-e67890f7f951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Drop the **employees2.csv** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "000b9311-6f41-4995-a99d-e6459458988b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Remove employees2.csv from the myfiles volume\n",
    "dbutils.fs.rm(f\"/Volumes/{DA.catalog_name}/{DA.schema_name}/myfiles/employees2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ae51bb7-edeb-4f45-ab90-088a8a8ee16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "DEWD00 - 02-Ingesting Data into Delta Lake",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
