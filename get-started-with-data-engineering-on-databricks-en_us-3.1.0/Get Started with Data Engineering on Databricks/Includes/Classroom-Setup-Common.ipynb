{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "374a66f4-c083-42e8-b1e7-ada061b946f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../Includes/_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df2ed1b7-8e7b-4000-af92-10d2504b166d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "DA = DBAcademyHelper()\n",
    "DA.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "437dc60e-605b-4efa-9ea5-3e6e3f2f8628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- CREATE DA VARIABLE USING SQL FOR USER INFORMATION FROM THE META TABLE\n",
    "\n",
    "-- Create a temp view storing information from the obs table.\n",
    "CREATE OR REPLACE TEMP VIEW user_info AS\n",
    "SELECT map_from_arrays(collect_list(replace(key,'.','_')), collect_list(value))\n",
    "FROM dbacademy.ops.meta;\n",
    "\n",
    "-- Create SQL dictionary var (map)\n",
    "DECLARE OR REPLACE DA MAP<STRING,STRING>;\n",
    "\n",
    "-- Set the temp view in the DA variable\n",
    "SET VAR DA = (SELECT * FROM user_info);\n",
    "\n",
    "DROP VIEW IF EXISTS user_info;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7dfd7f3-add0-4990-8fae-2002ed2f7f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c77fed61-1a28-4b8b-a765-95d591cdd8cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "volume_name = \"myfiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95ebf4d4-c8f1-493b-8d29-790ed9029c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_volume(self):\n",
    "    \"\"\"\n",
    "    Create a volume in the module's catalog using the specified volume_name.\n",
    "    \"\"\"\n",
    "    ## Specify the volume name\n",
    "    create_volume_location = f'{DA.catalog_name}.{DA.schema_name}.{volume_name}'\n",
    "\n",
    "    ## Create the volume\n",
    "    createVolume = f'CREATE VOLUME IF NOT EXISTS {create_volume_location}'\n",
    "    spark.sql(createVolume)\n",
    "    print(f'Your volume {self.__bold}{volume_name}{self.__reset} in {self.__bold}{self.catalog_name}.{self.schema_name}{self.__reset} is available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81634c2a-f190-4806-8b3a-b8408394e6bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_employees_csv(self):\n",
    "    \"\"\"Creates the employees.csv file in the specified catalog.Schema.Volume.\"\"\"\n",
    "    data = [\n",
    "        [\"1111\", \"Kristi\", \"USA\", \"Manager\"],\n",
    "        [\"2222\", \"Sophia\", \"Greece\", \"Developer\"],\n",
    "        [\"3333\", \"Peter\", \"USA\", \"Developer\"],\n",
    "        [\"4444\", \"Zebi\", \"Pakistan\", \"Administrator\"]\n",
    "    ]\n",
    "    columns = [\"ID\", \"Firstname\", \"Country\", \"Role\"]\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    file_path = f\"/Volumes/{self.catalog_name}/{self.schema_name}/{volume_name}/employees.csv\"\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"The employees.csv is created in your schema's {volume_name} volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "375a2501-2791-4144-9704-3141d93581b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_employees_csv2(self):\n",
    "    \"\"\"\n",
    "    Creates the employees2.csv file in the specified catalog.Schema.Volume.\n",
    "    \"\"\"\n",
    "    # Create data for the CSV file\n",
    "    data = [\n",
    "        [5555, 'Alex','USA', 'Instructor'],\n",
    "        [6666, 'Sanjay','India', 'Instructor']\n",
    "    ]\n",
    "    columns = [\"ID\",\"Firstname\", \"Country\", \"Role\"]\n",
    "\n",
    "    ## Create the DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    ## Create the CSV file in the course Catalog.Schema.Volume\n",
    "    df.to_csv(f\"/Volumes/{self.catalog_name}/{self.schema_name}/{volume_name}/employees2.csv\", index=False)\n",
    "\n",
    "    print(f\"The employees2.csv is created in your schema's {volume_name} volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f37bde7f-bd8d-478b-83ea-5f3716430995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def print_lakeflow_job_info(self):\n",
    "    path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
    "    newpath = path.replace('DEWD00 - 04-Creating a Simple Databricks Job','')\n",
    "    task1path = newpath + '/DEWD00 - 04A-Task 1 - Setup - Bronze'\n",
    "    task2path = newpath + '/DEWD00 - 04B-Task 2 - Silver - Gold'\n",
    "    \n",
    "    print(f'Name your LakeFlow Job: {self.__bold}{DA.schema_name}_Example{self.__reset}')\n",
    "    print(' ')\n",
    "    print(f'{self.__bold}NOTEBOOK PATHS FOR TASKS{self.__reset}')\n",
    "    print(f'- Task 1 notebook path: \\n{self.__bold}{task1path}{self.__reset}')\n",
    "    print(f'- Task 2 notebook path: \\n{self.__bold}{task2path}{self.__reset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab07c059-8526-4ede-9a2d-33d3d11142c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_taxi_files(self):\n",
    "    \"\"\"\n",
    "    Create the samples.nyctaxi.trips Delta as a csv file in the user's volume.\n",
    "    \"\"\"\n",
    "    spark.sql(f'CREATE VOLUME IF NOT EXISTS {DA.catalog_name}.{DA.schema_name}.taxi_files')\n",
    "    output_volume = f'/Volumes/{DA.catalog_name}/{DA.schema_name}/taxi_files'\n",
    "    \n",
    "\n",
    "    sdf = spark.table(\"samples.nyctaxi.trips\")\n",
    "    \n",
    "    (sdf\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .csv(output_volume, header=True)\n",
    "        )\n",
    "    \n",
    "    print(f'The taxi data is available in the {self.__bold}taxi_files{self.__reset} volume within the {self.__bold}{DA.catalog_name}{self.__reset} Catalog in your schema {self.__bold}{DA.schema_name}{self.__reset}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13527b15-4741-4e10-9065-a255c4573fbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def drop_employee_csv2(self):\n",
    "    \"\"\"\n",
    "    Drop the employee_csv2 file if it exists.\n",
    "    \"\"\"\n",
    "    file_path = f\"/Volumes/{DA.catalog_name}/{DA.schema_name}/{volume_name}/employees2.csv\"\n",
    "\n",
    "    try:\n",
    "        dbutils.fs.ls(file_path)  # Check if the file exists\n",
    "        dbutils.fs.rm(file_path)  # Delete the file if found\n",
    "    except Exception as e:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e975b51-7076-43c6-ba0b-50485db3b7c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def drop_taxi_volume(self):\n",
    "    \"\"\"\n",
    "    Drop the taxidata volume if exists.\n",
    "    \"\"\"\n",
    "    spark.sql('DROP volume if exists taxi_files')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-Common",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
